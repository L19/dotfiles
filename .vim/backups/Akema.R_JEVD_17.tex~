\documentclass{article}
\usepackage[dvipdfmx]{graphicx}
\usepackage{bm}
\usepackage[cmex10]{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}

\newtheorem{problem}{Problem}
\newtheorem{theorem}{Theorem}

\title{Joint Eigenvalue Decomposition}
\author{Riku Akema}
\date{\today}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}

\subsection{Stationary points of the Jacobi algorithms}
\label{sec:jacobi}
Suppose that $\{R_\tau\}_{\tau=1}^T\subset\mathbb{R}^{N\times N}$ in Problem \ref{prob:SD} constructed from symmetric matrices is simultaneous diagonalizable. The Jacobi algorithms reduce the quantity
\begin{align}
  \label{eq:offdiagonal}
  \sum_{\tau = 1}^{T} \mathrm{off}(R_\tau),
\end{align}
where $\mathrm{off}(\cdot):\mathbb{R}^{N \times N} \rightarrow \mathbb{R}$ is defined as
\begin{align*}
  \mathrm{off}(R_\tau) := \sum_{\substack{p, q = 1 \\ p \neq q}}^N (r^{(\tau)}_{p, q})^2
\end{align*}
with the $(p, q)$-th entry $r_{p, q}^{(\tau)}$ of $R_\tau$. By introducing an $N$-by-$N$ matrix
\begin{align*}
  &J(p, q, \theta)\\
  &\quad:=
  \left[
    \begin{array}{ccccccc}
      1&\cdots&0&\cdots&0&\cdots&0\\
      \rvdots&\ddots&\rvdots&&\rvdots&&\rvdots\\
      0&\cdots&\cos\theta&\cdots&\sin\theta&\cdots&0\\
      \rvdots&&\rvdots&\ddots&\rvdots&&\rvdots\\
      0&\cdots&-\sin\theta&\cdots&\cos\theta&\cdots&0\\
      \rvdots&&\rvdots&&\rvdots&\ddots&\rvdots\\
      0&\cdots&0&\cdots&0&\cdots&1
    \end{array}
  \right]\hspace{-3mm}
  \begin{array}{l}
    \\
    \\
    \leftarrow p\\
    \\
    \leftarrow q\\
    \\
    \\
  \end{array}
  \\
  &\hspace{2.2cm}
  \begin{array}{ccccccc}
    &&\uparrow&\hspace{1.1cm}&\uparrow&&\\
    &&p&\hspace{1.1cm}&q&&
  \end{array}
%   \begin{split}
%     \bordermatrix{
%       & & & p & & q & & \\
%       & 1 & \cdots & 0 & \cdots & 0 & \cdots & 0 \\
%       & \vdots & \ddots & \vdots & & \vdots & & \vdots \\
%       p & 0 & \cdots & \cos \theta & \cdots & \sin \theta & \cdots & 0 \\
%       & \vdots & & \vdots & \ddots & \vdots & & \vdots \\
%       q & 0 & \cdots & -\sin \theta & \cdots & \cos \theta & \cdots & 0 \\
%       & \vdots & & \vdots & & \vdots & \ddots & \vdots \\
%       & 0 & \cdots & 0 & \cdots & 0 & \cdots & 1
%     }
%   \end{split}
%   \in \mathbb{R}^{N \times N},
\end{align*}
with $\theta\in[0,2\pi)$, the basic step of the Jacobi algorithms is represented as
\begin{align}
  \label{eq:jacobistep}
  (\forall \tau \in \{1, 2, \ldots , T\}) \quad R'_\tau = J(p, q, \theta)^\top R_\tau J(p, q, \theta).
\end{align}
Since the Frobenius norm of $R_\tau$ is preserved by orthogonal transformations, the following
relation is hold
\begin{align}
  & \sum_{\tau = 1}^T \left( \mathrm{off}(R'_\tau) + (r'^{(\tau)}_{p, p})^2 + (r'^{(\tau)}_{q, q})^2 \right) \nonumber\\
  &\quad= \sum_{\tau = 1}^T \left( \mathrm{off}(R_\tau) + (r^{(\tau)}_{p, p})^2 + (r^{(\tau)}_{q, q})^2 \right)\nonumber\\
  \Leftrightarrow & \sum^T_{\tau = 1} \mathrm{off}(R'_\tau) \nonumber\\
                  &\quad= \sum_{\tau = 1}^T \left( \mathrm{off}(R_\tau) + (r^{(\tau)}_{p, p})^2 + (r^{(\tau)}_{q, q})^2\right.\nonumber\\
                  &\hspace{3.5cm}\left.- (r'^{(\tau)}_{p, p})^2 - (r'^{(\tau)}_{q, q})^2 \right).\label{eq:costdiff1}
\end{align}
By using the two equations: $2\left((r'^{(\tau)}_{p, p})^2 + (r'^{(\tau)}_{q, q})^2\right)=(r'^{(\tau)}_{p, p} + r'^{(\tau)}_{q, q})^2+(r'^{(\tau)}_{p, p} - r'^{(\tau)}_{q, q})^2$ and $r'^{(\tau)}_{p, p} + r'^{(\tau)}_{q, q}=r^{(\tau)}_{p, p} + r^{(\tau)}_{q, q}$,
\begin{align}
  \eqref{eq:costdiff1}\Leftrightarrow&\ \sum_{\tau = 1}^T \left( \mathrm{off}(R_\tau) + \frac{1}{2}(r^{(\tau)}_{p, p} - r^{(\tau)}_{q, q})^2\right.\nonumber\\
                  &\hspace{3cm}\left.-\frac{1}{2}(r'^{(\tau)}_{p, p} - r'^{(\tau)}_{q, q})^2 \right).\label{eq:costdiff2}
\end{align}
Hence, minimization of \eqref{eq:offdiagonal} is seen, by \eqref{eq:costdiff2}, to be equivalent to maximization
of $\sum^T_{\tau = 1} (r'^{(\tau)}_{p, p} - r'^{(\tau)}_{q, q})^2$. Moreover, from \eqref{eq:jacobistep},
\begin{align}
  r'^{(\tau)}_{p, p} - r'^{(\tau)}_{q, q} & = (\cos^2 \theta - \sin^2 \theta)(r_{p, p}^{(\tau)} - r_{q, q}^{(\tau)})\nonumber\\
                                          &\quad+ 4 \cos \theta \sin \theta r_{p, q}\nonumber\\
                                          & = \left[ 
  \begin{array}{c}
    \cos^2 \theta - \sin^2 \theta \\
    2 \cos \theta \sin \theta
  \end{array}
  \right]^\top \left[ \begin{array}{c}
    r_{p, p}^{(\tau)} - r_{q, q}^{(\tau)} \\
    2 r^{(\tau)}_{p, q}
  \end{array}
  \right]\nonumber\\
  & =: \bm{u}(\theta)^\top \bm{h}(R_\tau)\nonumber.
\end{align}
Then, the parameter $\theta$ is chosen to maximize the following quadratic form
\begin{align}
  & \sum^T_{\tau = 1} \left( \bm{u}(\theta)^\top \bm{h}(R_\tau) \right)^2 \nonumber\\ 
  &\quad=\sum^T_{\tau=1}\bm{u}(\theta)^\top\bm{h}(R_\tau)\bm{h}(R_\tau)^\top\bm{u}(\theta)\label{eq:jacobiQuadForm}\\
  &\quad= \,\bm{u}(\theta)^\top \left[ \begin{array}{cc}
    a & b \\
    b & c
  \end{array}
  \right] \bm{u}(\theta)\nonumber,
\end{align}
where
\begin{align}
  \left\{
    \begin{array}{l}
      a=\displaystyle{\sum^T_{\tau = 1}} (r_{p, p}^{(\tau)} - r_{q, q}^{(\tau)})^2,\\
      b=2 \displaystyle{\sum^T_{\tau = 1}} r_{p, q}^{(\tau)}(r_{p, p}^{(\tau)} - r_{q, q}^{(\tau)}),\\
      c=4 \displaystyle{\sum^T_{\tau = 1}} {r_{p, q}^{(\tau)}}^2.
    \end{array}
  \right.
\end{align}
Since $\|\bm{u}(\theta)\|_2=1$, the maxima of \eqref{eq:jacobiQuadForm} is known to be a maximum eigenvalue of $\bm{h}(R_\tau)\bm{h}(R_\tau)^\top$. Therefore the solution $\theta$ is given by any unit norm eigenvector associated with this eigenvalue.

\section{Joint Diagonalization with a High Dimensional Eigenvalue Problem}
Let $\mathbb{R}$ and $\mathbb{N}$ denote the set of all real numbers and the set of all nonnegative integers, respectively. We use $(\cdot)^{\top}$ to denote the transpose of vector or matrix.

For given $T$ matrices $\{R_{\tau}\}_{\tau=1}^{T}\subset \mathbb{R}^{N\times N}$ that can be diagonalized by a common invertible matrix $M_{\star}\in\mathbb{R}^{N\times N}$, i.e., 
\begin{align}
	\label{eq: jointDiag}
	\left.
	\begin{array}{l}
		R_{\tau}=M_{\star}\Sigma_{\tau}M_{\star}^{-1}\quad (\tau=1,2,\ldots ,T), \\
		\Sigma_{\tau}(\in\mathbb{R}^{N\times N}){\rm \ is\ a\ diagonal\ matrix.}
	\end{array}
	\right\},
\end{align}
we consider a joint diagonalization problem:
\begin{problem}
	\label{prob: jointDiagProb}
	For given $T$ matrices $\{R_{\tau}\}_{\tau=1}^{T}\subset \mathbb{R}^{N\times N}$ satisfying \eqref{eq: jointDiag}, find an invertible matrix $M\in\mathbb{R}^{N\times N}$ such that $M^{-1}R_{\tau}M$ is diagonal.
\end{problem}
%Note that, in general, more than $3$ symmetric positive semi-definite matrices have no any invertible matrix that diagonalize the all matrices simultaneously.

If $N = 2$ as well as $M_{\star}$ is a orthogonal matrix, the CWSOBI mechanism essentially provides a closed-form solution of Problem \ref{prob: jointDiagProb}. Meanwhile, if $N>2$, its closed-form solution is unknown.

In this report, to facilitate the search of $M$ in Problem \ref{prob: jointDiagProb}, we investigate a necessary condition of all solutions of Problem \ref{prob: jointDiagProb}. Our analysis shows that a certain vector constructed from each columns of $M$ must be an eigenvector of a matrix $Q\in\mathbb{R}^{N^{2}\times N^{2}}$ constructed from all $\{R_{\tau}\}_{\tau=1}^{T}$. This implies that solving eigenvalue problem of $Q$ provides candidates of solutions of Problem \ref{prob: jointDiagProb}. In addition, we apply its solution to blind source separation with $N$ sensors and $N$ signals.

Denote the $j$-th row vector of $R_{\tau}$ by $\bm{r}^{(\tau)}_{j}\in\mathbb{R}^{1\times N}$, similarly, the $j$-th row vector of $M$ by $\bm{m}_{j,:}\in\mathbb{R}^{1\times N}$, where $j\in\{1,2,\ldots ,N\}$. In addition, we introduce a vector $\bm{\sigma}_{\tau}\in\mathbb{R}^{1\times N}$ satisfying that ${\rm diag}(\bm{\sigma}_{\tau}) = \Sigma_{\tau}$, where $\tau\in\{1,2,\ldots ,T\}$. Describing the $j$-th row of $R_{\tau}$ yields
\begin{align}
	\label{eq: j-thRow1}
	(M\ {\rm is\ a\ solutions\ of\ Problem\ \ref{prob: jointDiagProb}}) & \Leftrightarrow (\forall j\in\{1, 2, \ldots ,N\})(\forall \tau\in\{1,2, \ldots ,T\})\ \bm{r}_{j}^{(\tau)}=\bm{m}_{j,:}\Sigma_{\tau}M^{-1}.
\end{align}
Since $\Sigma_{\tau}$ is a diagonal matrix,
\begin{align}
	\label{eq: j-thRow2}
	\eqref{eq: j-thRow1} \Leftrightarrow (\forall j\in\{1, 2, \ldots ,N\})(\forall \tau\in\{1,2, \ldots ,T\})\ \bm{r}_{j}^{(\tau)}=\bm{\sigma}_{\tau}\,{\rm diag}(\bm{m}_{j,:})M^{-1}.
\end{align}
By introducing a matrix $\hat{R}_{j}:=[(\bm{r}_{j}^{(1)})^{\top},(\bm{r}_{j}^{(2)})^{\top},\ldots ,(\bm{r}_{j}^{(T)})^{\top}]^{\top}\in\mathbb{R}^{T\times N}$ whose the $\tau$-th row vector is $\bm{r}_{j}^{(\tau)}$, similarly, a matrix $\hat{S}:=[(\bm{\sigma}_{j})^{\top},(\bm{\sigma}_{j})^{\top},\ldots ,(\bm{\sigma}_{j})^{\top}]^{\top}\in\mathbb{R}^{T\times N}$ whose the $\tau$-th row vector is $\bm{\sigma}_{\tau}$, we have
\begin{align}
	\eqref{eq: j-thRow2} & \Leftrightarrow (\forall j\in\{1, 2, \ldots ,N\})\ \hat{R}_{j}=\hat{S}\,{\rm diag}(\bm{m}_{j,:})M^{-1}\\
	\label{eq: j-thMat1}
	& \Leftrightarrow (\forall j\in\{1, 2, \ldots ,N\})\ \hat{R}_{j}M=\hat{S}\,{\rm diag}(\bm{m}_{j,:}).
\end{align}
Describing the $i$-th column vector of both sides of \eqref{eq: j-thMat1} yields
\begin{align}
	\label{eq: j-thMat2}
	\eqref{eq: j-thMat1}\Leftrightarrow (\forall i,j\in\{1, 2, \ldots ,N\})\ \hat{R}_{j}\bm{m}_{i}=\hat{\bm{s}}_{i}m_{j,i},
\end{align}
where $\bm{m}_{i}\in\mathbb{R}^{N}$ is the $i$-th column vector of $M$, $m_{j,i}\in\mathbb{R}$ is the $(j,i)$ element of $M$, and $\hat{\bm{s}}_{i}\in\mathbb{R}^{T}$ is the $i$-th column vector of $\hat{S}$. 

Now, Using the equation of \eqref{eq: j-thMat2} with $\hat{R}_{j}$ and $\hat{R}_{k}\ (k=1,2,\ldots ,N)$, we construct the $(j + (k - 1)N)$-th row vector of $Q\in\mathbb{R}^{N^{2}\times N^{2}}$. Considering quadratic form of $\hat{R}_{k}^{\top}\hat{R}_{j}$, we have
\begin{align}
	\label{eq: quadForm1}
	\eqref{eq: j-thMat2} \Rightarrow (\forall i,j,k\in\{1,2,\ldots ,N\})\ \bm{m}_{i}^{\top}\hat{R}_{k}^{\top}\hat{R}_{j}\bm{m}_{i} = m_{k,i}m_{j,i}\|\hat{\bm{s}}_{i}\|^{2}_{2}.
\end{align}
Note that the reason why the equation of \eqref{eq: quadForm1} is necessary condition of \eqref{eq: j-thMat2} is that we cannot reproduce $\hat{R}_{k}\bm{m}_{i},\hat{R}_{j}\bm{m}_{i}\in\mathbb{R}^{T}$ uniquely from $\bm{m}_{i}^{\top}\hat{R}_{k}^{\top}\hat{R}_{j}\bm{m}_{i} = (\hat{R}_{k}\bm{m}_{i})^{\top}\hat{R}_{j}\bm{m}_{i}$.
\begin{align}
	\eqref{eq: quadForm1} &\Leftrightarrow (\forall i,j,k\in\{1,2,\ldots ,N\})\ (\hat{R}_{k}\bm{m}_{i})^{\top}\hat{R}_{j}\bm{m}_{i} = m_{j,i}m_{k,i}\|\hat{\bm{s}}_{i}\|^{2}_{2} \\
	&\Leftrightarrow (\forall i,j,k\in\{1,2,\ldots ,N\})\ {\rm tr}(\hat{R}_{j}\bm{m}_{i}(\hat{R}_{k}\bm{m}_{i})^{\top}) = m_{j,i}m_{k,i}\|\hat{\bm{s}}_{i}\|^{2}_{2} \\
	& \Leftrightarrow (\forall i,j,k\in\{1,2,\ldots ,N\})\ {\rm tr}(\hat{R}_{j}\bm{m}_{i}\bm{m}_{i}^{\top}\hat{R}_{k}^{\top}) = m_{j,i}m_{k,i}\|\hat{\bm{s}}_{i}\|^{2}_{2} \\
	& \Leftrightarrow (\forall i,j,k\in\{1,2,\ldots ,N\})\ {\rm tr}(\hat{R}_{k}^{\top}\hat{R}_{j}\bm{m}_{i}\bm{m}_{i}^{\top}) = m_{j,i}m_{k,i}\|\hat{\bm{s}}_{i}\|^{2}_{2}\\
	& \Leftrightarrow (\forall i,j,k\in\{1,2,\ldots ,N\})\ {\rm tr}((\hat{R}_{j}^{\top}\hat{R}_{k})^{\top}\bm{m}_{i}\bm{m}_{i}^{\top}) = m_{j,i}m_{k,i}\|\hat{\bm{s}}_{i}\|^{2}_{2}\\
	\label{eq: quadForm2}
	& \Leftrightarrow (\forall i,j,k\in\{1,2,\ldots ,N\})\ {\rm vec}(\hat{R}_{j}^{\top}\hat{R}_{k})^{\top}\,{\rm vec}(\bm{m}_{i}\bm{m}_{i}^{\top}) = m_{j,i}m_{k,i}\|\hat{\bm{s}}_{i}\|^{2}_{2}.
\end{align}
Constructing $Q$ by using all ${\rm vec}(\hat{R}_{j}^{\top}\hat{R}_{k})^{\top}$, we have
\begin{align}
	\eqref{eq: quadForm2} \Leftrightarrow (\forall i\in\{1,2,\ldots ,N\})\ Q\ {\rm vec}(\bm{m}_{i}\bm{m}_{i}^{\top}) = \|\hat{\bm{s}}_{i}\|^{2}_{2}\ {\rm vec}(\bm{m}_{i}\bm{m}_{i}^{\top}).
\end{align}
The last equation implies that ${\rm vec}(\bm{m}_{i}\bm{m}_{i}^{\top})$ is an eigenvector of $Q$ and its associate eigenvalue is $\|\hat{\bm{s}}_{i}\|^{2}_{2}$.

Consequently, our necessary condition is as follows:
\begin{theorem}
	\label{theo: JDviaED}
	Suppose that $M\in\mathbb{R}^{N\times N}$ is a solution of Problem \ref{prob: jointDiagProb}. Define $\bm{v}_{i} =\ {\rm vec}(\bm{m}_{i}\bm{m}_{i}^{\top})\in\mathbb{R}^{N^{2}}\ (i=1,2,\ldots ,N)$ with the $i$-th column vector $\bm{m}_{i}\in\mathbb{R}^{N}$ of $M$. Then there exists a matrix $Q\in\mathbb{R}^{N^{2}\times N^{2}}$ constructed from all $\{R_{\tau}\}_{\tau=1}^{T}\subset\mathbb{R}^{N\times N}$ in Problem \ref{prob: jointDiagProb} such that $\bm{v}_{i}$ must be an eigenvector of $Q$, i.e.,
	\begin{align*}
		Q\bm{v}_{i}=\lambda_{i}\bm{v}_{i}
	\end{align*}
with some $\lambda_{i}\ge0$.
\end{theorem}

\section{Reformulating of Blind Source Separation Problem to Problem \ref{prob: jointDiagProb}}
\subsection{Blind Source Separation Problem Formulation}
Denote ensemble averaging operator by $\mathbb{E}[\cdot]$. We suppose that $N$ sensors receive $N$ mixed signals. The vector of observed data snapshots can be modeled as 
\begin{align}
	\label{eq: model}
	\bm{x}_{t} = A \bm{s}_{t} + \bm{n}_{t} \in \mathbb{R}^{N},
\end{align}
where $t\in\mathbb{N}$ denotes the time index, $A\in\mathbb{R}^{N\times N}$ is an unknown full-rank mixing matrix, $\bm{s}_{t} := [s_{t}^{(1)}, s_{t}^{(2)}, \ldots , s_{t}^{(N)}]^{\top}\in\mathbb{R}^{N}$ is zero mean random process under the assumptions given by
\begin{description}
	\item[(wide-sence stationarity)]\mbox{}\\
		\begin{align}
			\label{eq: signalAssumption1}
			\mathbb{E}[s_{t+\tau}^{(i)}s_{t}^{(i)}] = \sigma_{\tau}^{(i)}\in\mathbb{R}\quad (i=1,2, \ldots ,N,\ \tau\in\mathbb{N})
		\end{align}
	\item[(mutual uncorrelation)]\mbox{}\\
		\begin{align}
			\label{eq: signalAssumption2}
			\mathbb{E}[s_{t+\tau}^{(i)}s_{t}^{(j)}] = 0\quad (i, j=1,2, \ldots ,N,\ i\neq j,\ \tau\in\mathbb{N}),
		\end{align}
\end{description}
 and $\bm{n}_{t} := [n_{t}^{(1)}, n_{t}^{(2)}, \ldots , n_{t}^{(N)}]^{\top}\in\mathbb{R}^{N}$ is zero mean random process under the assumptions given by
\begin{description}
	\item[(temporary and spatially white)]\mbox{}\\
		\begin{align}
			\label{eq: noiseAssumption1}
			\mathbb{E}[n_{t+\tau}^{(i)}n_{t}^{(j)}] = \left\{
			\begin{array}{ll}
				\sigma^{2} \ge 0 & {\rm if}\ i = j,\ \tau = 0, \\
				0 & {\rm otherwise}
			\end{array}
			\right. \quad (i,j=1,2,\ldots ,N,\ \tau\in\mathbb{N})
		\end{align}
	\item[(uncorrelation with signals)]\mbox{}\\
		\begin{align}
			\label{eq: noiseAssumption2}
			\mathbb{E}[s_{k+\tau_{1}}^{(i)}n_{k+\tau_{2}}^{(j)}] =0\quad (i=1,2, \ldots ,N,\ j=1,2,\ldots ,N,\ \tau_{1}, \tau_{2}\in\mathbb{N}).
		\end{align}
\end{description}

The major goal of blind source separation is to identify the mixed signals without determinacy of a permutation and a scaling ambiguity, i.e., to estimate $D\Pi\bm{s}_{t}$ from $\bm{x}_{t}$ with a permutation matrix $\Pi\in\mathbb{R}^{N\times N}$ and a diagonal matrix $D\in\mathbb{R}^{N\times N}$, without any prior knowledge on $A$.

\subsection{Reformulating to Problem \ref{prob: jointDiagProb}}
The covariance matrices of the observed data snapshots is represented by
\begin{align*}
	R_{x}(\tau)&:=\mathbb{E}[\bm{x}_{t+\tau}\bm{x}_{t}^{\top}]\nonumber\\
	&=\mathbb{E}[(A\bm{s}_{t+\tau}+\bm{n}_{t+\tau})(A\bm{s}_{t}+\bm{n}_{t})^{\top}]\nonumber\\
	&=A\mathbb{E}[\bm{s}_{t+\tau}\bm{s}_{t}^{\top}]A^{\top} + A\mathbb{E}[\bm{s}_{t+\tau}\bm{n}_{t}^{\top}] + \mathbb{E}[\bm{n}_{t+\tau}\bm{s}_{t}^{\top}]A^{\top} + \mathbb{E}[\bm{n}_{t+\tau}\bm{n}_{t}^{\top}].
\end{align*}
Using the signal assumptions \eqref{eq: signalAssumption1} and \eqref{eq: signalAssumption2}, we have
\begin{align*}
	\mathbb{E}[\bm{s}_{t+\tau}\bm{s}_{t}^{\top}] =\ {\rm diag}([\sigma^{(1)}_{\tau},\sigma^{(2)}_{\tau},\ldots ,\sigma^{(N)}_{\tau}])\quad(\tau\in\mathbb{N}).
\end{align*}
In addition, using the noise assumptions \eqref{eq: noiseAssumption1} and \eqref{eq: noiseAssumption2}, we have
\begin{align*}
	\left.
	\begin{array}{l}
		\mathbb{E}[\bm{s}_{t+\tau}\bm{n}_{t}^{\top}]=\mathbb{E}[\bm{n}_{t+\tau}\bm{s}_{t}^{\top}]=0, \\
		\mathbb{E}[\bm{n}_{t+\tau}\bm{n}_{t}^{\top}]=\left\{
		\begin{array}{ll}
			\sigma^{2}I_{N} & {\rm if}\ \tau=0,\\
			0 & {\rm otherwise}.
		\end{array}
		\right.
	\end{array}
	\right\} \quad(\tau\in\mathbb{N}),
\end{align*}
where $I_{N}\in\mathbb{R}^{N\times N}$ denotes the identity matrix. Then,
\begin{align}
	\label{eq: dataCovariance}
	R_{x}(\tau)=\left\{
	\begin{array}{ll}
		A\,{\rm diag}([\sigma_{0}^{(1)},\sigma_{0}^{(2)},\ldots ,\sigma_{0}^{(N)}])A^{\top}+\sigma^{2}I_{N} & {\rm if}\ \tau=0, \\
		A\,{\rm diag}([\sigma_{\tau}^{(1)},\sigma_{\tau}^{(2)},\ldots ,\sigma_{\tau}^{(N)}])A^{\top} & {\rm otherwise}
	\end{array}
	\right.\quad (\tau\in\mathbb{N})
\end{align}
where $\sigma_{0}^{(i)}>0\ (i=1,2,\ldots , N)$. 


\section{Optimality Condition for a Minimizer of the Off-diagonal Cost Function}
\label{sec:optimality}
For given $T$ matrices $\{R_\tau\} \subset \mathbb{R}^{N \times N}$, we consider the following problem:
\begin{align}
  \underset{M \in GL_N(\mathbb{R})}{\operatorname{minimize}}\ \sum_{\tau = 1}^T
  \|\mathrm{off}(M^{-1}R_\tau M)\|^2_F,
\end{align}
where $\mathrm{off}:\mathbb{R}^{N \times N} \rightarrow \mathbb{R}^{N \times N}:X \mapsto X - \mathrm{diag}(X)$.
From continuity of polynomial roots, there exists a some sufficiently small $\varepsilon > 0$ 
s.t. $B_N(M, \varepsilon) \subset GL_N(\mathbb{R})$. Since for any $\epsilon \in B_N(O,
\varepsilon):\|M^{-1} \epsilon\|_2 < 1$, Neumann series provides
\begin{align}
  (M + \epsilon)^{-1} & = (I_N + M^{-1}\epsilon)^{-1}M^{-1} \nonumber \\
                      & = \left(I_N - M^{-1}\epsilon - \sum_{n = 2}^\infty (M^{-1}\epsilon)^n \right)M^{-1} \nonumber \\
  \label{eq:neumann}
                      & = M^{-1} - M^{-1}\epsilon M^{-1} - o(\epsilon). 
\end{align}
Define $f_\tau:\mathbb{R}^{N \times N} \rightarrow \mathbb{R}:M \mapsto \|\mathrm{off}(M^{-1}R_\tau M)\|^2_F\ 
(\tau = 1, 2, \ldots ,T)$ Then \eqref{eq:neumann} results in for each $\tau = 1, 2, \ldots T$
\begin{align}
  f_\tau (M + \epsilon) & = \|\mathrm{off}\left((M + \epsilon)^{-1}R_\tau (M + \epsilon)\right)\|^2_F \nonumber \\
                        & = \|\mathrm{off}\left((M^{-1} - M^{-1}\epsilon M^{-1} - o(\epsilon))R_\tau (M + \epsilon)\right) \nonumber \\
                        & = \| \|
\end{align}

\section{Noise Reduction}
Define $\tilde{R}_\tau\ (\tau = 1, 2, \ldots , T)$ as
\begin{align}
  (\forall \tau \in \{1, 2, \ldots , T\})\ \tilde{R}_\tau & := R_\tau + E_\tau \nonumber \\
                                                         & = M \Sigma_\tau M^{-1} + E_\tau, 
\end{align}
where $M \in GL_N(\mathbb{R})$, $\Sigma_\tau \in \mathbb{R}$ is a diagonal matrix, and $E_\tau \in \mathbb{R}$
is a sparse matrix. Vectorizing $\tilde{R}_\tau$ with column-wise for any $\tau = 1, 2, \ldots ,T$ and stacking 
them as column vector of a matrix $\tilde{R} \in \mathbb{R}^{N^2 \times N^2}$, we have
\begin{align}
  \tilde{R} & := \left[ \operatorname{vec}(\tilde{R}_1), \operatorname{vec}(\tilde{R}_2), \ldots , \operatorname{vec}(\tilde{R}_T) \right] \nonumber \\
    & = (M^{-\top} \diamond M) \left[ \sigma_1, \sigma_2, \ldots , \sigma_T \right] + \left[ \operatorname{vec}(E_1), \operatorname{vec}(E_2), \ldots , \operatorname{vec}(E_T) \right] \nonumber \\
    & = R_\star + E_\star \in \mathbb{R}^{N^2 \times T},
\end{align}
where $\diamond$ denotes Khatri-Rao product (i.e. column-wise kronecker product), $\sigma_\tau := \operatorname{diag}(\Sigma_\tau) \in \mathbb{R}^N\ (\tau = 1, 2, \ldots , T)$, $R_\star := (M^{-\top} \diamond M)[\sigma_1, \sigma_2, \ldots , \sigma_T] \in \mathbb{R}^{N^2 \times T}$, and $E_\star := [\operatorname{vec}(E_1), \operatorname{vec}(E_2), \ldots , \operatorname{vec}(E_T)] \in \mathbb{R}^{N^2 \times T}$. Since $T \gg N$, $\operatorname{rank}(R) = N (< N^2)$. Therefore, we estimate $R_\star$ and $E_\star$ by solving the following problem
\begin{align}
  \underset{R, E \in \mathbb{R}^{N^2 \times T}}{\operatorname{minimize}}\ \|R\|_\ast + \rho\|E\|_1, \quad \text{s.t. } \tilde{R} = R + E, 
\end{align}
where $\rho > 0$, $\|\cdot\|_\ast$ denotes the nuclear norm, and $\|\cdot\|_1$ denotes sum of absolute elements of a matrix. 

\end{document}
